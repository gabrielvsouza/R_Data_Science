Statistical Inference
https://rafalab.github.io/dsbook/inference.html

the part of Statistics that helps distinguish patterns arising from signal from those arising from chance. 
Statistical inference is a broad topic and here we go over the very basics using polls as a motivating examples. 
To described the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.

Understand how to use a sampling model to perform a poll.
Explain the terms population, parameter, and sample as they relate to statistical inference.
Use a sample to estimate the population proportion from the sample average.
Calculate the expected value and standard error of the sample average. 

Sampling Model Parameters and Estimates
To help us understand the connection between polls and the probability theory that we have learned, let’s construct 
a scenario that we can work through together and that is similar to the one that pollsters face.
We will use an urn instead of voters. And because pollsters are competing with other pollsters for media attention, 
we will imitate that by having our competition with a $25 prize. The challenge is to guess the spread between the 
proportion of blue and red balls in this urn. Before making a prediction, you can take a sample, with replacement, 
from the urn.
To mimic the fact that running polls is expensive, it will cost you $0.10 per bead you sample. 
So if your sample size is 250 and you win, you’ll break even, as you’ll have to pay me $25 to collect your $25.
Your entry into the competition can be an interval. If the interval you submit contains the true proportion, 
you get half what you paid and pass to the second phase of the competition.
In the second phase of the competition, the entry with the smallest interval is selected as the winner.
The dslabs package includes a function that shows a random draw from the urn that we just saw. 
Here’s the code that you can write to see a sample.
    library(tidyverse)
    library(dslabs)
    ds_theme_set()
    # And here is a sample with 25 beads.
    take_poll(25)

OK, now that you know the rules, think about how you would construct your interval. How many beads would you sample, 
et cetera.
Notice that we have just described a simple sampling model for opinion polls. 
The beads inside the urn represent the individuals that will vote on election day. 
Those that will vote Republican are represented with red beads and the Democrats with blue beads. 
For simplicity, assume there are no other colors, that there are just two parties.

We want to predict the proportion of blue beads in the urn. Let’s call this quantity p, which in turn tells us the 
proportion of red beads, 1−p, and the spread, p−(1−p), which simplifies to 2p−1.
In statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the 
population, p, is called a parameter. 
The 25 beads that we saw in an earlier plot after we sampled, that’s called a sample.

The task of statistical inference is to predict the parameter, p, using the observed data in the sample. 
Now, can we do this with just the 25 observations we showed you?
Well, they are certainly informative. For example, given that we see 13 red and 12 blue, it is unlikely that 
p is bigger than 0.9 or smaller than 0.1. Because if they were, it would be un-probable to see 13 red and 12 blue. 
But are we ready to predict with certainty that there are more red beads than blue.
OK, what we want to do is construct an estimate of p using only the information we observe. 
An estimate can be thought of as a summary of the observed data that we think is informative about the parameter 
of interest. It seems intuitive to think that the proportion of blue beads in the sample, which in this case is 0.48, 
must be at least related to the actual proportion p. But do we simply predict p to be 0.48?
First, note that the sample proportion is a random variable. If we run the command take_poll(25), 
say four times, we get four different answers. Each time the sample is different and the sample proportion is different. 
The sample proportion is a random variable.
    par(mfrow = c(2, 2))
    take_poll(25)
    take_poll(25)

    take_poll(25)
    take_poll(25)
Note that in the four random samples we show, the sample proportion ranges from 0.44 to 0.6. 
By describing the distribution of this random variable, we’ll be able to gain insights into how good 
this estimate is and how we can make it better.



###Populations, samples, parameters and estimates
We want to predict the proportion of blue beads in the urn. 
Let’s call this quantity: p - proportion of blue beads
which then tells us the proportion of red beads: 1 - p
and the spread: p - (1 - p), which simplifies to:  2p - 1

In statistical textbooks, the beads in the urn are called the population. 
The proportion of blue beads in the population p is called a parameter
The 25 beads we see in the previous plot are called a sample. 
The task of statistical inference is to predict the parameter p
using the observed data in the sample.
Can we do this with the 25 observations above? 

We want to construct an estimate of  p
using only the information we observe. 
An estimate should be thought of as a summary of the observed data that we think is informative 
about the parameter of interest. 

For example, given that we see 13 red and 12 blue beads
It seems intuitive to think that the proportion of blue beads in the sample  
0.48 p must be at least related to the actual proportion
But do we simply predict  p to be 0.48?
Remember that the sample proportion is a random variable

If we run the command take_poll(25) four times, 
we get a different answer each time, since the sample proportion is a random variable
Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. 
By describing the distribution of this random variable, 
we will be able to gain insights into how good this estimate is and how we can make it better.



###The Sample Average
Conducting an opinion poll is being modeled as taking a random sample from an urn
aking an opinion poll is being modeled as taking a random sample from an urn. 
We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter p. 
Once we have this estimate, we can easily report an estimate of the spread: 2p−1.

But for simplicity, we will illustrate the concept of statistical inference for estimating p. We will 
use our knowledge of probability to defend our use of the sample proportion, and quantify how close we think
it is from the population proportion p.

We start by defining the random variable X. X is going to be 1 if we pick a blue bead at random, and 0 if it’s red. 
This implies that we’re assuming that the population, the beads in the urn, are a list of 0s and 1s.

If we sample N beads, then the average of the draws X1 through XN is equivalent to the proportion of blue beads
in our sample. This is because adding the Xs is equivalent to counting the blue beads, and dividing by the 
total N turns this into a proportion. We use the symbol x̅ to represent this average. In general, in statistics textbooks,
a bar on top of a symbol means the average.

The theory we just learned about the sum of draws becomes useful, because we know the distribution of the
sum N times X-bar. We know the distribution of the average X-bar, because N is a non random constant.


For simplicity, let’s assume that the draws are independent. After we see each sample bead, we return it to the urn. 
It’s a sample with replacement. In this case, what do we know about the distribution of the sum of draws?

First, we know that the expected value of the sum of draws is N times the average of the values in the urn. 
We know that the average of the 0s and 1s in the urn must be the proportion p, the value we want to estimate.

Here, we encounter an important difference with what we did in the probability module. 
We don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. 
This is what we’re trying to find out. We’re trying to estimate p.

Just like we use variables to define unknowns in systems of equations, in statistical inference, 
we define parameters to define unknown parts of our models. 
In the urn model we are using to mimic an opinion poll, 
we do not know the proportion of blue beads in the urn. We define the parameter p to represent this quantity. 

We are going to estimate this parameter.

Note that the ideas presented here, on how we estimate parameters and provide insights into how good these estimates are, 
extrapolate to many data science tasks.

For example, we may ask, 

what is the difference in health improvement between patients receiving treatment and a control group?
We may ask, what are the health effects of smoking on a population? 
What are the differences in racial groups of fatal shootings by police? 
What is the rate of change in life expectancy in the US during the last 10 years?

All these questions can be framed as a task of estimating a parameter from a sample.



###Polling versus Forecasting
Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. 
If a poll is conducted four months before the election, it is estimating the p for that moment and not for election day.



### Properties of our estimate: expected value and standard error
Using what we have learned, the expected value of the sum N times X-bar is N times the average of the urn, p.
  E(NX¯)=N×p

Dividing by the nonrandom constant N gives us that the expected value of the average X-bar is p. 
We can write it using our mathematical notation like this.
  E( X¯) = p

The standard error of the average is square root of p times 1 minus p divided by the square root of N.
  SE(X¯)= SQRT(p(1−p)/N)

The expected value of the sample proportion, X-bar, is the parameter of interest, p. E(X¯)=p

And we can make the standard error as small as we want by increasing the sample size, N. SE(X¯)= SQRT(p(1−p)/N)

The law of large numbers tells us that, with a large enough poll, our estimate converges to p.
If we take a large enough poll to make our standard error, say, about 0.01, we’ll be quite certain about who will win.

But how large does a pool have to be for the standard error to be this small? One problem is that we do not know p, 
so we can’t actually compute the standard error.

For illustrative purposes, let’s assume that p is 0.51 and make a plot of the standard error versus the sample size N. 
Here it is.

You can see that obviously it’s dropping. From the plot, we also see that we would need a poll of over 10,000 people 
to get the standard error as low as we want it to be. We rarely see polls of this size due, in part, to costs. 
We’ll give other reasons later.

From the RealClearPolitics table we saw earlier, we learned that the sample sizes in opinion polls range from 500 to 3,500. 
For a sample size of 1,000, if we set p to be 0.51, the standard error is about 0.15, or 1.5 percentage points.

So even with large polls, for close elections, X-bar can lead us astray if we don’t realize it’s a random variable.

But, we can actually say more about how close we can get to the parameter p.

Exercise 1. Polling - expected value of S
Suppose you poll a population in which a proportion p of voters are Democrats and 1-p are Republicans. 
Your sample size is N=25. Consider the random variable S, which is the total number of Democrats in your sample.
What is the expected value of this random variable S?
Answer:
E(S)=25p

Exercise 2. Polling - standard error of S
Again, consider the random variable S, which is the total number of Democrats in your sample of 25 voters. 
The variable p describes the proportion of Democrats in the sample, whereas 1-p describes the proportion of Republicans.
What is the standard error of S?
Answer:
SE(S)= sqrt(25p(1−p))

Exercise 3. Polling - expected value of X¯
Consider the random variable S/N, which is equivalent to the sample average that we have been denoting as X¯. 
The variable N represents the sample size and p is the proportion of Democrats in the population.
What is the expected value of X¯?
Answer:
E(X¯)=p

Exercise 4. Polling - standard error of X¯
What is the standard error of the sample average, X¯?
The variable N represents the sample size and p is the proportion of Democrats in the population.
Answer:
SE(X¯)= SQRT(p(1−p)/N)

Exercise 5. se versus p
Write a line of code that calculates the standard error se of a sample average when you poll 25 people in the population. Generate a sequence of 100 proportions of Democrats p that vary from 0 (no Democrats) to 1 (all Democrats).
Plot se versus p for the 100 different proportions.
Instructions
  Use the seq function to generate a vector of 100 values of p that range from 0 to 1.
  Use the sqrt function to generate a vector of standard errors for all values of p.
  Use the plot function to generate a plot with p on the x-axis and se on the y-axis.
Answer:
# N represents the number of people polled 
    N <- 25 
# Create a variable p that contains 100 proportions ranging from 0 to 1 using the seq function p <- seq(0, 1, length.out=100) 
    p <- seq(0, 1, length = 100)
# Create a variable se that contains the standard error of each sample average se <- sqrt(p * (1-p) / N) 
    se <- sqrt(p*(1-p)/N)
# Plot p on the x-axis and se on the y-axis plot(p, se)
    plot(p, se) 

Exercise 6. Multiple plots of se versus p
Using the same code as in the previous exercise, create a for-loop that generates three plots of p versus se 
when the sample sizes equal N=25, N=100, and N=1000.
Instructions
Your for-loop should contain two lines of code to be repeated for three different values of N.
The first line within the for-loop should use the sqrt function to generate a vector of 
standard errors se for all values of p.
The second line within the for-loop should use the plot function to generate a plot with p on the x-axis
and se on the y-axis.
Use the ylim argument to keep the y-axis limits constant across all three plots. 
The lower limit should be equal to 0 and the upper limit should equal the highest calculated standard error across all values of p and N.
Answer:
# The vector p contains 100 proportions of Democrats ranging from 0 to 1 using the seq function 
    p <- seq(0, 1, length = 100) 
# The vector  sample_sizes contains the three sample sizes 
    sample_sizes <- c(25, 100, 1000) 
# Write a for-loop that calculates the standard error 'se' for every value of p for each of the
three samples sizes N in the vector sample_sizes. 
Plot the three graphs, using the ylim argument to standardize the y-axis across all three plots. 
    for(samples in sample_sizes){
      se <- sqrt(p*(1-p)/samples)
      plot(p,se,ylim=c(0, 0.1))
    }

Exercise 7. Expected value of d
Our estimate for the difference in proportions of Democrats and Republicans is d=X¯−(1−X¯).
Which derivation correctly uses the rules we learned about sums of random variables and 
scaled random variables to derive the expected value of  d?
Answer:
  E[X¯−(1−X¯)]=E[2X¯−1] =2E[X¯]−1 =2p−1 =p−(1−p)

Exercise 8. Standard error of d
Our estimate for the difference in proportions of Democrats and Republicans is d=X¯−(1−X¯).
Which derivation correctly uses the rules we learned about sums of random variables and 
scaled random variables to derive the standard error of  d?
Answer:
  SE[X¯−(1−X¯)]=SE[2X¯−1] =2SE[X¯] = SQRT(p(1−p)/N)

Exercise 9. Standard error of the spread
Say the actual proportion of Democratic voters is p=0.45. 
In this case, the Republican party is winning by a relatively large margin of d=−0.1, or a 10% margin of victory. 
What is the standard error of the spread 2X¯−1 in this case?
Instructions
Use the sqrt function to calculate the standard error of the spread 2X¯−1.
Answer:
# N represents the number of people polled 
    N <- 25 
# p represents the proportion of Democratic voters 
    p <- 0.45 
# Calculate the standard error of the spread. Print this value to the console. 
    2 * sqrt(p * (1-p) / N)
[1] 0.1989975


Exercise 10. Sample size
So far we have said that the difference between the proportion of Democratic voters and Republican voters 
is about 10% and that the standard error of this spread is about 0.2 when N=25. 
Select the statement that explains why this sample size is sufficient or not.
Answer:
    This sample size is too small because the standard error is larger than the spread.



### Central Limit Theorem in practice

Use the Central Limit Theorem to calculate the probability that a sample estimate  X¯  is close to the population proportion  p .
Run a Monte Carlo simulation to corroborate theoretical results built using probability theory.
Estimate the spread based on estimates of  X¯  and  SE^(X¯) .
Understand why bias can mean that larger sample sizes aren't necessarily better.
There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

The central limit theorem tells us that the distribution function for a sum of draws
is approximately normal. We also learned that when dividing a normally
distributed random variable by a nonrandom constant,
the resulting random variable is also normally distributed.
This implies that the distribution of X-bar is approximately normal.

So in summary, we have that X-bar has an approximately normal distribution.
And in a previous video, we determined that the expected value is p,
and the standard error is the square root of p times 1 minus p
divided by the sample size N.
       se <- sqrt(p*(1-p)/N) 
       
Now, how does this help us?
Suppose we want to know what is the probability that we
are within one percentage point (1%) from p that we
made a very, very good estimate?

So we're basically asking, what's the probability
that the distance between X-bar and p, the absolute value of X-bar minus p,
is less than 0.01, 1 percentage point.
      Pr(X¯ - p| <= 0.01)


In our first sample we had 12 blue and 13 red so  
¯X=0.48 and our estimate of standard error is:
    x_hat <- 0.48
    se <- sqrt(x_hat*(1-x_hat)/25)
    se
    #> [1] 0.0999
And now we can answer the question of the probability of being close to  
p. The answer is:
    pnorm(0.01/se) - pnorm(-0.01/se)
    #> [1] 0.0797

Therefore, there is a small chance that we will be close. 
A poll of only N = 25 people is not really very useful, at least not for a close election.
Earlier we mentioned the margin of error. N
ow we can define it because it is simply two times the standard error, which we can now estimate. 
In our case it is:
    1.96*se
    #> [1] 0.196

Why do we multiply by 1.96? 
Because if you ask what is the probability that we are within 1.96 standard errors from p
which we know is about 95%:
    pnorm(1.96)-pnorm(-1.96)
    #> [1] 0.95
 
In summary, the CLT tells us that our poll based on a sample size of 15 is not very useful. 
We don’t really learn much when the margin of error is this large. 
All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.
From the table above, we see that typical sample sizes range from 700 to 3500.
 
A Monte Carlo Simulation for the CLT
    B <- 10000
    N <- 1000
    x_hat <- replicate(B, {
      x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
      mean(x)
    })
 
The problem is, of course, we don’t know p
One thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations.
Let’s set p=0.45. We can then simulate a poll:
    p <- 0.45
    N <- 1000

    x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    x_hat <- mean(x) 
 We can use that code to do a Monte Carlo simulation:
     B <- 10000
    x_hat <- replicate(B, {
      x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
      mean(x)
    })

To review, the theory tells us that that  ¯X is approximately normally distributed, 
has expected value p=0.45 standard error SE(X¯)= SQRT(p(1−p)/N) = 0.016
The simulation confirms this:
    mean(x_hat)
    #> [1] 0.45
    sd(x_hat)
    #> [1] 0.0157

Again, note that in real life, we would never
be able to run such an experiment because we don't know p.
But we could run it for various values of p and sample sizes N
and see that the theory does indeed work well for most values.
You can easily do this yourself by rerunning the code we showed you
after changing p and N.



### The spread



### Bias: why not run a very large poll?
Note that for realistic values of p, say between 0.35 and 0.65
for the popular vote, if we run a very large poll with say 100,000 people,
theory would tell us that we would predict the election almost perfectly,
since the largest possible margin of error is about 0.3%.
    N <- 100000
    p <- seq(0.35, 0.65, lenght =100)
    SE <- saplly (p, function(x) 2*sqrt(x*(1-x)/N))
    data.frame(p=p, SE = SE) %>% ggplot(aes(p, SE)) +
        geom_line()

So why are there no pollsters that are conducting polls this large?
One reason is that running polls with a sample size of 100,000
is very expensive.

But perhaps a more important reason is that theory has its limitations.
Polling is much more complicated than picking beads from an urn.
For example, while the beads are either red or blue,
and you can see it with your eyes, people, when you ask them,
might lie to you.

Also, because you're conducting these polls usually by phone,
you might miss people that don't have phones.
And they might vote differently than those that do.

But perhaps the most different way an actual poll is from our urn model
is that we actually don't know for sure who is in our population and who is not.
How do we know who is going to vote?
Are we reaching all possible voters?

So, even if our margin of error is very small,
it may not be exactly right that our expected value is p.
We call this bias.

Historically, we observe that polls are, indeed, biased,
although not by that much.
The typical bias appears to be between 1% and 2%.
This makes election forecasting a bit more interesting.
And we'll talk about that in a later video.



Exercise 1. Sample average¶
Write function called take_sample that takes the proportion of Democrats p and the sample size N
as arguments and returns the sample average of Democrats (1) and Republicans (0).
Calculate the sample average if the proportion of Democrats equals 0.45 and the sample size is 100.
Create a vector using c() that contains all possible polling options.
Use replace = TRUE within the sample function to indicate that sampling from the vector should occur with replacement.
Use prob = within the sample function to indicate the probabilities of selecting either element within the vector of possibilities.
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
    take_sample <- function(p, N){
      mean(sample(0:1, N, replace=T, prob=c(1-p,p)))
    }     
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
    take_sample(p,N)

Exercise 2. Distribution of errors - 1¶
Assume the proportion of Democrats in the population p equals 0.45 and that your sample size N is 100 polled voters. 
The take_sample function you defined previously generates our estimate, X¯
Replicate the random sampling 10,000 times and calculate p−X¯ for each random sample. Save these differences as a vector called errors. Find the average of errors and plot a histogram of the distribution.
https://www.rdocumentation.org/packages/Rpdb/versions/2.2/topics/replicate
The function take_sample that you defined in the previous exercise has already been run for you.
Use the replicate function to replicate subtracting the result of take_sample from the value of p 10,000 times.
Use the mean function to calculate the average of the differences between the sample average and actual value of p.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
    errors <- replicate(B, {p - take_sample(p,N)})
# Calculate the mean of the errors. Print this value to the console.
    mean(errors)

Exercise 3. Distribution of errors - 2
In the last exercise, you made a vector of differences between the actual value for p and an estimate, X¯
We called these differences between the actual and estimated values errors.
The errors object has already been loaded for you. Use the hist function to plot a histogram of the values 
contained in the vector errors. Which statement best describes the distribution of the errors?
hist(errors)
3 The errors are symmetrically distributed around 0.

Exercise 4. Average size of error
The error p−X¯ is a random variable. In practice, the error is not observed because we do not know 
the actual proportion of Democratic voters, p
However, we can describe the size of the error by constructing a simulation.
What is the average size of the error if we define the size by taking the absolute value ∣p−X¯∣ ?
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
    # Define `p` as the proportion of Democrats in the population being polled
p <- 0.45
# Define `N` as the number of people polled
N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))
# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))

Exercise 5. Standard deviation of the spread
The standard error is related to the typical size of the error we make when predicting. 
We say size because, as we just saw, the errors are centered around 0. 
In that sense, the typical error is 0. For mathematical reasons related to the central limit theorem, 
we actually use the standard deviation of errors rather than the average of the absolute values.
As we have discussed, the standard error is the square root of the average squared distance (X¯−p)2
The standard deviation is defined as the square root of the distance squared.
Calculate the standard deviation of the spread.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Calculate the standard deviation of `errors`
    sqrt(mean(errors^2))    

Exercise 6. Estimating the standard error¶
The theory we just learned tells us what this standard deviation is going to be because it is the standard error of X¯
Estimate the standard error given an expected value of 0.45 and a sample size of 100.
# Define `p` as the expected value equal to 0.45
    p <- 0.45
# Define `N` as the sample size
    N <- 100
# Calculate the standard error
    sqrt(p*(1-p)/N)


Exercise 7. Standard error of the estimate
In practice, we don't know p, so we construct an estimate of the theoretical prediction 
based by plugging in X¯ for p. Calculate the standard error of the estimate:
SE^(X¯)
Simulate a poll X using the sample function.
When using the sample function, create a vector using c() that contains all possible polling options where '1' indicates a Democratic voter and '0' indicates a Republican voter.
When using the sample function, use replace = TRUE within the sample function to indicate that sampling from the vector should occur with replacement.
When using the sample function, use prob = within the sample function to indicate the probabilities of selecting either element (0 or 1) within the vector of possibilities.
Use the mean function to calculate the average of the simulated poll, X_bar.
Calculate the standard error of the X_bar using the sqrt function and print the result.
# Define `p` as a proportion of Democratic voters to simulate
    p <- 0.45
# Define `N` as the sample size
    N <- 100
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
    X <- sample(0:1, N, replace=T, p=c(1-p,p))
# Define `X_bar` as the average sampled proportion
    X_bar <- mean(X)
# Calculate the standard error of the estimate. Print the result to the console.
    sqrt(X_bar*(1-X_bar)/N)


Exercise 8. Plotting the standard error¶
The standard error estimates obtained from the Monte Carlo simulation, the theoretical prediction, 
and the estimate of the theoretical prediction are all very close, which tells us that the theory is working. 
This gives us a practical approach to knowing the typical error we will make if we predict p with X^. 
he theoretical result gives us an idea of how large a sample size is required to obtain the precision we need. 
Earlier we learned that the largest standard errors occur for p=0.5
Create a plot of the largest standard error for N ranging from 100 to 5,000. 
Based on this plot, how large does the sample size have to be to have a standard error of about 1%?
    N <- seq(100, 5000, len = 100) 
    p <- 0.5 
    e <- sqrt(p*(1-p)/N)
2,500

Exercise 11. Plotting the errors
Make a qq-plot of the errors you generated previously to see if they follow a normal distribution.
Run the supplied code
Use the qqnorm function to produce a qq-plot of the errors.
Use the qqline function to plot a line showing a normal distribution.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Generate `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Generate a qq-plot of `errors` with a qq-line showing a normal distribution
    qqnorm(errors)
    qqline(errors)

Exercise 12. Estimating the probability of a specific value of X-bar¶
If p=0.45 and N=100, use the central limit theorem to estimate the probability that X¯>0.5.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# Calculate the probability that the estimated proportion of Democrats in the population is greater than 0.5. Print this value to the console.
    1-pnorm(0.5, mean = p, sd=(sqrt(p*(1-p)/N)))

Exercise 13. Estimating the probability of a specific error size
Assume you are in a practical situation and you don't know p . 
Take a sample of size N=100 and obtain a sample average of X¯=0.51
What is the CLT approximation for the probability that your error is equal or larger than 0.01?
Calculate the standard error of the sample average using the sqrt function.
Use pnorm twice to define the probabilities that a value will be less than 0.01 or -0.01.
Calculate the probability that the error will be 0.01 or larger.
# Define `N` as the number of people polled
    N <-100
# Define `X_hat` as the sample average
    X_hat <- 0.51
# Define `se_hat` as the standard error of the sample average
    se_hat <- sqrt(X_hat*(1-X_hat)/N)
# Calculate the probability that the error is 0.01 or larger
    1 - pnorm(.01, 0, se_hat) + pnorm(-0.01, 0, se_hat)



### Confidence Intervals
Calculate confidence intervals of difference sizes around an estimate.
Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.
Explain the concept of "power" as it relates to inference.
Understand the relationship between p-values and confidence intervals and explain why 
reporting confidence intervals is often preferable.
































































