Statistical Inference
https://rafalab.github.io/dsbook/inference.html

the part of Statistics that helps distinguish patterns arising from signal from those arising from chance. 
Statistical inference is a broad topic and here we go over the very basics using polls as a motivating examples. 
To described the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.

Understand how to use a sampling model to perform a poll.
Explain the terms population, parameter, and sample as they relate to statistical inference.
Use a sample to estimate the population proportion from the sample average.
Calculate the expected value and standard error of the sample average. 

Sampling Model Parameters and Estimates
To help us understand the connection between polls and the probability theory that we have learned, let’s construct 
a scenario that we can work through together and that is similar to the one that pollsters face.
We will use an urn instead of voters. And because pollsters are competing with other pollsters for media attention, 
we will imitate that by having our competition with a $25 prize. The challenge is to guess the spread between the 
proportion of blue and red balls in this urn. Before making a prediction, you can take a sample, with replacement, 
from the urn.
To mimic the fact that running polls is expensive, it will cost you $0.10 per bead you sample. 
So if your sample size is 250 and you win, you’ll break even, as you’ll have to pay me $25 to collect your $25.
Your entry into the competition can be an interval. If the interval you submit contains the true proportion, 
you get half what you paid and pass to the second phase of the competition.
In the second phase of the competition, the entry with the smallest interval is selected as the winner.
The dslabs package includes a function that shows a random draw from the urn that we just saw. 
Here’s the code that you can write to see a sample.
    library(tidyverse)
    library(dslabs)
    ds_theme_set()
    # And here is a sample with 25 beads.
    take_poll(25)

OK, now that you know the rules, think about how you would construct your interval. How many beads would you sample, 
et cetera.
Notice that we have just described a simple sampling model for opinion polls. 
The beads inside the urn represent the individuals that will vote on election day. 
Those that will vote Republican are represented with red beads and the Democrats with blue beads. 
For simplicity, assume there are no other colors, that there are just two parties.

We want to predict the proportion of blue beads in the urn. Let’s call this quantity p, which in turn tells us the 
proportion of red beads, 1−p, and the spread, p−(1−p), which simplifies to 2p−1.
In statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the 
population, p, is called a parameter. 
The 25 beads that we saw in an earlier plot after we sampled, that’s called a sample.

The task of statistical inference is to predict the parameter, p, using the observed data in the sample. 
Now, can we do this with just the 25 observations we showed you?
Well, they are certainly informative. For example, given that we see 13 red and 12 blue, it is unlikely that 
p is bigger than 0.9 or smaller than 0.1. Because if they were, it would be un-probable to see 13 red and 12 blue. 
But are we ready to predict with certainty that there are more red beads than blue.
OK, what we want to do is construct an estimate of p using only the information we observe. 
An estimate can be thought of as a summary of the observed data that we think is informative about the parameter 
of interest. It seems intuitive to think that the proportion of blue beads in the sample, which in this case is 0.48, 
must be at least related to the actual proportion p. But do we simply predict p to be 0.48?
First, note that the sample proportion is a random variable. If we run the command take_poll(25), 
say four times, we get four different answers. Each time the sample is different and the sample proportion is different. 
The sample proportion is a random variable.
    par(mfrow = c(2, 2))
    take_poll(25)
    take_poll(25)

    take_poll(25)
    take_poll(25)
Note that in the four random samples we show, the sample proportion ranges from 0.44 to 0.6. 
By describing the distribution of this random variable, we’ll be able to gain insights into how good 
this estimate is and how we can make it better.



###Populations, samples, parameters and estimates
We want to predict the proportion of blue beads in the urn. 
Let’s call this quantity: p - proportion of blue beads
which then tells us the proportion of red beads: 1 - p
and the spread: p - (1 - p), which simplifies to:  2p - 1

In statistical textbooks, the beads in the urn are called the population. 
The proportion of blue beads in the population p is called a parameter
The 25 beads we see in the previous plot are called a sample. 
The task of statistical inference is to predict the parameter p
using the observed data in the sample.
Can we do this with the 25 observations above? 

We want to construct an estimate of  p
using only the information we observe. 
An estimate should be thought of as a summary of the observed data that we think is informative 
about the parameter of interest. 

For example, given that we see 13 red and 12 blue beads
It seems intuitive to think that the proportion of blue beads in the sample  
0.48 p must be at least related to the actual proportion
But do we simply predict  p to be 0.48?
Remember that the sample proportion is a random variable

If we run the command take_poll(25) four times, 
we get a different answer each time, since the sample proportion is a random variable
Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. 
By describing the distribution of this random variable, 
we will be able to gain insights into how good this estimate is and how we can make it better.



###The Sample Average
Conducting an opinion poll is being modeled as taking a random sample from an urn
aking an opinion poll is being modeled as taking a random sample from an urn. 
We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter p. 
Once we have this estimate, we can easily report an estimate of the spread: 2p−1.

But for simplicity, we will illustrate the concept of statistical inference for estimating p. We will 
use our knowledge of probability to defend our use of the sample proportion, and quantify how close we think
it is from the population proportion p.

We start by defining the random variable X. X is going to be 1 if we pick a blue bead at random, and 0 if it’s red. 
This implies that we’re assuming that the population, the beads in the urn, are a list of 0s and 1s.

If we sample N beads, then the average of the draws X1 through XN is equivalent to the proportion of blue beads
in our sample. This is because adding the Xs is equivalent to counting the blue beads, and dividing by the 
total N turns this into a proportion. We use the symbol x̅ to represent this average. In general, in statistics textbooks,
a bar on top of a symbol means the average.

The theory we just learned about the sum of draws becomes useful, because we know the distribution of the
sum N times X-bar. We know the distribution of the average X-bar, because N is a non random constant.


For simplicity, let’s assume that the draws are independent. After we see each sample bead, we return it to the urn. 
It’s a sample with replacement. In this case, what do we know about the distribution of the sum of draws?

First, we know that the expected value of the sum of draws is N times the average of the values in the urn. 
We know that the average of the 0s and 1s in the urn must be the proportion p, the value we want to estimate.

Here, we encounter an important difference with what we did in the probability module. 
We don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. 
This is what we’re trying to find out. We’re trying to estimate p.

Just like we use variables to define unknowns in systems of equations, in statistical inference, 
we define parameters to define unknown parts of our models. 
In the urn model we are using to mimic an opinion poll, 
we do not know the proportion of blue beads in the urn. We define the parameter p to represent this quantity. 

We are going to estimate this parameter.

Note that the ideas presented here, on how we estimate parameters and provide insights into how good these estimates are, 
extrapolate to many data science tasks.

For example, we may ask, 

what is the difference in health improvement between patients receiving treatment and a control group?
We may ask, what are the health effects of smoking on a population? 
What are the differences in racial groups of fatal shootings by police? 
What is the rate of change in life expectancy in the US during the last 10 years?

All these questions can be framed as a task of estimating a parameter from a sample.



###Polling versus Forecasting
Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. 
If a poll is conducted four months before the election, it is estimating the p for that moment and not for election day.



### Properties of our estimate: expected value and standard error
Using what we have learned, the expected value of the sum N times X-bar is N times the average of the urn, p.
  E(NX¯)=N×p

Dividing by the nonrandom constant N gives us that the expected value of the average X-bar is p. 
We can write it using our mathematical notation like this.
  E( X¯) = p

The standard error of the average is square root of p times 1 minus p divided by the square root of N.
  SE(X¯)= SQRT(p(1−p)/N)

The expected value of the sample proportion, X-bar, is the parameter of interest, p. E(X¯)=p

And we can make the standard error as small as we want by increasing the sample size, N. SE(X¯)= SQRT(p(1−p)/N)

The law of large numbers tells us that, with a large enough poll, our estimate converges to p.
If we take a large enough poll to make our standard error, say, about 0.01, we’ll be quite certain about who will win.

But how large does a pool have to be for the standard error to be this small? One problem is that we do not know p, 
so we can’t actually compute the standard error.

For illustrative purposes, let’s assume that p is 0.51 and make a plot of the standard error versus the sample size N. 
Here it is.

You can see that obviously it’s dropping. From the plot, we also see that we would need a poll of over 10,000 people 
to get the standard error as low as we want it to be. We rarely see polls of this size due, in part, to costs. 
We’ll give other reasons later.

From the RealClearPolitics table we saw earlier, we learned that the sample sizes in opinion polls range from 500 to 3,500. 
For a sample size of 1,000, if we set p to be 0.51, the standard error is about 0.15, or 1.5 percentage points.

So even with large polls, for close elections, X-bar can lead us astray if we don’t realize it’s a random variable.

But, we can actually say more about how close we can get to the parameter p.

Exercise 1. Polling - expected value of S
Suppose you poll a population in which a proportion p of voters are Democrats and 1-p are Republicans. 
Your sample size is N=25. Consider the random variable S, which is the total number of Democrats in your sample.
What is the expected value of this random variable S?
Answer:
E(S)=25p

Exercise 2. Polling - standard error of S
Again, consider the random variable S, which is the total number of Democrats in your sample of 25 voters. 
The variable p describes the proportion of Democrats in the sample, whereas 1-p describes the proportion of Republicans.
What is the standard error of S?
Answer:
SE(S)= sqrt(25p(1−p))

Exercise 3. Polling - expected value of X¯
Consider the random variable S/N, which is equivalent to the sample average that we have been denoting as X¯. 
The variable N represents the sample size and p is the proportion of Democrats in the population.
What is the expected value of X¯?
Answer:
E(X¯)=p

Exercise 4. Polling - standard error of X¯
What is the standard error of the sample average, X¯?
The variable N represents the sample size and p is the proportion of Democrats in the population.
Answer:
SE(X¯)= SQRT(p(1−p)/N)

Exercise 5. se versus p
Write a line of code that calculates the standard error se of a sample average when you poll 25 people in the population. Generate a sequence of 100 proportions of Democrats p that vary from 0 (no Democrats) to 1 (all Democrats).
Plot se versus p for the 100 different proportions.
Instructions
  Use the seq function to generate a vector of 100 values of p that range from 0 to 1.
  Use the sqrt function to generate a vector of standard errors for all values of p.
  Use the plot function to generate a plot with p on the x-axis and se on the y-axis.
Answer:
# N represents the number of people polled 
    N <- 25 
# Create a variable p that contains 100 proportions ranging from 0 to 1 using the seq function p <- seq(0, 1, length.out=100) 
    p <- seq(0, 1, length = 100)
# Create a variable se that contains the standard error of each sample average se <- sqrt(p * (1-p) / N) 
    se <- sqrt(p*(1-p)/N)
# Plot p on the x-axis and se on the y-axis plot(p, se)
    plot(p, se) 

Exercise 6. Multiple plots of se versus p
Using the same code as in the previous exercise, create a for-loop that generates three plots of p versus se 
when the sample sizes equal N=25, N=100, and N=1000.
Instructions
Your for-loop should contain two lines of code to be repeated for three different values of N.
The first line within the for-loop should use the sqrt function to generate a vector of 
standard errors se for all values of p.
The second line within the for-loop should use the plot function to generate a plot with p on the x-axis
and se on the y-axis.
Use the ylim argument to keep the y-axis limits constant across all three plots. 
The lower limit should be equal to 0 and the upper limit should equal the highest calculated standard error across all values of p and N.
Answer:
# The vector p contains 100 proportions of Democrats ranging from 0 to 1 using the seq function 
    p <- seq(0, 1, length = 100) 
# The vector  sample_sizes contains the three sample sizes 
    sample_sizes <- c(25, 100, 1000) 
# Write a for-loop that calculates the standard error 'se' for every value of p for each of the
three samples sizes N in the vector sample_sizes. 
Plot the three graphs, using the ylim argument to standardize the y-axis across all three plots. 
    for(samples in sample_sizes){
      se <- sqrt(p*(1-p)/samples)
      plot(p,se,ylim=c(0, 0.1))
    }

Exercise 7. Expected value of d
Our estimate for the difference in proportions of Democrats and Republicans is d=X¯−(1−X¯).
Which derivation correctly uses the rules we learned about sums of random variables and 
scaled random variables to derive the expected value of  d?
Answer:
  E[X¯−(1−X¯)]=E[2X¯−1] =2E[X¯]−1 =2p−1 =p−(1−p)

Exercise 8. Standard error of d
Our estimate for the difference in proportions of Democrats and Republicans is d=X¯−(1−X¯).
Which derivation correctly uses the rules we learned about sums of random variables and 
scaled random variables to derive the standard error of  d?
Answer:
  SE[X¯−(1−X¯)]=SE[2X¯−1] =2SE[X¯] = SQRT(p(1−p)/N)

Exercise 9. Standard error of the spread
Say the actual proportion of Democratic voters is p=0.45. 
In this case, the Republican party is winning by a relatively large margin of d=−0.1, or a 10% margin of victory. 
What is the standard error of the spread 2X¯−1 in this case?
Instructions
Use the sqrt function to calculate the standard error of the spread 2X¯−1.
Answer:
# N represents the number of people polled 
    N <- 25 
# p represents the proportion of Democratic voters 
    p <- 0.45 
# Calculate the standard error of the spread. Print this value to the console. 
    2 * sqrt(p * (1-p) / N)
[1] 0.1989975


Exercise 10. Sample size
So far we have said that the difference between the proportion of Democratic voters and Republican voters 
is about 10% and that the standard error of this spread is about 0.2 when N=25. 
Select the statement that explains why this sample size is sufficient or not.
Answer:
    This sample size is too small because the standard error is larger than the spread.



### Central Limit Theorem in practice

Use the Central Limit Theorem to calculate the probability that a sample estimate  X¯  is close to the population proportion  p .
Run a Monte Carlo simulation to corroborate theoretical results built using probability theory.
Estimate the spread based on estimates of  X¯  and  SE^(X¯) .
Understand why bias can mean that larger sample sizes aren't necessarily better.
There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

The central limit theorem tells us that the distribution function for a sum of draws
is approximately normal. We also learned that when dividing a normally
distributed random variable by a nonrandom constant,
the resulting random variable is also normally distributed.
This implies that the distribution of X-bar is approximately normal.

So in summary, we have that X-bar has an approximately normal distribution.
And in a previous video, we determined that the expected value is p,
and the standard error is the square root of p times 1 minus p
divided by the sample size N.
       se <- sqrt(p*(1-p)/N) 
       
Now, how does this help us?
Suppose we want to know what is the probability that we
are within one percentage point (1%) from p that we
made a very, very good estimate?

So we're basically asking, what's the probability
that the distance between X-bar and p, the absolute value of X-bar minus p,
is less than 0.01, 1 percentage point.
      Pr(X¯ - p| <= 0.01)


In our first sample we had 12 blue and 13 red so  
¯X=0.48 and our estimate of standard error is:
    x_hat <- 0.48
    se <- sqrt(x_hat*(1-x_hat)/25)
    se
    #> [1] 0.0999
And now we can answer the question of the probability of being close to  
p. The answer is:
    pnorm(0.01/se) - pnorm(-0.01/se)
    #> [1] 0.0797

Therefore, there is a small chance that we will be close. 
A poll of only N = 25 people is not really very useful, at least not for a close election.
Earlier we mentioned the margin of error. N
ow we can define it because it is simply two times the standard error, which we can now estimate. 
In our case it is:
    1.96*se
    #> [1] 0.196

Why do we multiply by 1.96? 
Because if you ask what is the probability that we are within 1.96 standard errors from p
which we know is about 95%:
    pnorm(1.96)-pnorm(-1.96)
    #> [1] 0.95
 
In summary, the CLT tells us that our poll based on a sample size of 15 is not very useful. 
We don’t really learn much when the margin of error is this large. 
All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.
From the table above, we see that typical sample sizes range from 700 to 3500.
 
A Monte Carlo Simulation for the CLT
    B <- 10000
    N <- 1000
    x_hat <- replicate(B, {
      x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
      mean(x)
    })
 
The problem is, of course, we don’t know p
One thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations.
Let’s set p=0.45. We can then simulate a poll:
    p <- 0.45
    N <- 1000

    x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    x_hat <- mean(x) 
 We can use that code to do a Monte Carlo simulation:
     B <- 10000
    x_hat <- replicate(B, {
      x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
      mean(x)
    })

To review, the theory tells us that that  ¯X is approximately normally distributed, 
has expected value p=0.45 standard error SE(X¯)= SQRT(p(1−p)/N) = 0.016
The simulation confirms this:
    mean(x_hat)
    #> [1] 0.45
    sd(x_hat)
    #> [1] 0.0157

Again, note that in real life, we would never
be able to run such an experiment because we don't know p.
But we could run it for various values of p and sample sizes N
and see that the theory does indeed work well for most values.
You can easily do this yourself by rerunning the code we showed you
after changing p and N.



### The spread



### Bias: why not run a very large poll?
Note that for realistic values of p, say between 0.35 and 0.65
for the popular vote, if we run a very large poll with say 100,000 people,
theory would tell us that we would predict the election almost perfectly,
since the largest possible margin of error is about 0.3%.
    N <- 100000
    p <- seq(0.35, 0.65, lenght =100)
    SE <- saplly (p, function(x) 2*sqrt(x*(1-x)/N))
    data.frame(p=p, SE = SE) %>% ggplot(aes(p, SE)) +
        geom_line()

So why are there no pollsters that are conducting polls this large?
One reason is that running polls with a sample size of 100,000
is very expensive.

But perhaps a more important reason is that theory has its limitations.
Polling is much more complicated than picking beads from an urn.
For example, while the beads are either red or blue,
and you can see it with your eyes, people, when you ask them,
might lie to you.

Also, because you're conducting these polls usually by phone,
you might miss people that don't have phones.
And they might vote differently than those that do.

But perhaps the most different way an actual poll is from our urn model
is that we actually don't know for sure who is in our population and who is not.
How do we know who is going to vote?
Are we reaching all possible voters?

So, even if our margin of error is very small,
it may not be exactly right that our expected value is p.
We call this bias.

Historically, we observe that polls are, indeed, biased,
although not by that much.
The typical bias appears to be between 1% and 2%.
This makes election forecasting a bit more interesting.
And we'll talk about that in a later video.



Exercise 1. Sample average¶
Write function called take_sample that takes the proportion of Democrats p and the sample size N
as arguments and returns the sample average of Democrats (1) and Republicans (0).
Calculate the sample average if the proportion of Democrats equals 0.45 and the sample size is 100.
Create a vector using c() that contains all possible polling options.
Use replace = TRUE within the sample function to indicate that sampling from the vector should occur with replacement.
Use prob = within the sample function to indicate the probabilities of selecting either element within the vector of possibilities.
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
    take_sample <- function(p, N){
      mean(sample(0:1, N, replace=T, prob=c(1-p,p)))
    }     
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
    take_sample(p,N)

Exercise 2. Distribution of errors - 1¶
Assume the proportion of Democrats in the population p equals 0.45 and that your sample size N is 100 polled voters. 
The take_sample function you defined previously generates our estimate, X¯
Replicate the random sampling 10,000 times and calculate p−X¯ for each random sample. Save these differences as a vector called errors. Find the average of errors and plot a histogram of the distribution.
https://www.rdocumentation.org/packages/Rpdb/versions/2.2/topics/replicate
The function take_sample that you defined in the previous exercise has already been run for you.
Use the replicate function to replicate subtracting the result of take_sample from the value of p 10,000 times.
Use the mean function to calculate the average of the differences between the sample average and actual value of p.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
    errors <- replicate(B, {p - take_sample(p,N)})
# Calculate the mean of the errors. Print this value to the console.
    mean(errors)

Exercise 3. Distribution of errors - 2
In the last exercise, you made a vector of differences between the actual value for p and an estimate, X¯
We called these differences between the actual and estimated values errors.
The errors object has already been loaded for you. Use the hist function to plot a histogram of the values 
contained in the vector errors. Which statement best describes the distribution of the errors?
hist(errors)
3 The errors are symmetrically distributed around 0.

Exercise 4. Average size of error
The error p−X¯ is a random variable. In practice, the error is not observed because we do not know 
the actual proportion of Democratic voters, p
However, we can describe the size of the error by constructing a simulation.
What is the average size of the error if we define the size by taking the absolute value ∣p−X¯∣ ?
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
    # Define `p` as the proportion of Democrats in the population being polled
p <- 0.45
# Define `N` as the number of people polled
N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))
# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))

Exercise 5. Standard deviation of the spread
The standard error is related to the typical size of the error we make when predicting. 
We say size because, as we just saw, the errors are centered around 0. 
In that sense, the typical error is 0. For mathematical reasons related to the central limit theorem, 
we actually use the standard deviation of errors rather than the average of the absolute values.
As we have discussed, the standard error is the square root of the average squared distance (X¯−p)2
The standard deviation is defined as the square root of the distance squared.
Calculate the standard deviation of the spread.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Calculate the standard deviation of `errors`
    sqrt(mean(errors^2))    

Exercise 6. Estimating the standard error¶
The theory we just learned tells us what this standard deviation is going to be because it is the standard error of X¯
Estimate the standard error given an expected value of 0.45 and a sample size of 100.
# Define `p` as the expected value equal to 0.45
    p <- 0.45
# Define `N` as the sample size
    N <- 100
# Calculate the standard error
    sqrt(p*(1-p)/N)


Exercise 7. Standard error of the estimate
In practice, we don't know p, so we construct an estimate of the theoretical prediction 
based by plugging in X¯ for p. Calculate the standard error of the estimate:
SE^(X¯)
Simulate a poll X using the sample function.
When using the sample function, create a vector using c() that contains all possible polling options where '1' indicates a Democratic voter and '0' indicates a Republican voter.
When using the sample function, use replace = TRUE within the sample function to indicate that sampling from the vector should occur with replacement.
When using the sample function, use prob = within the sample function to indicate the probabilities of selecting either element (0 or 1) within the vector of possibilities.
Use the mean function to calculate the average of the simulated poll, X_bar.
Calculate the standard error of the X_bar using the sqrt function and print the result.
# Define `p` as a proportion of Democratic voters to simulate
    p <- 0.45
# Define `N` as the sample size
    N <- 100
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
    X <- sample(0:1, N, replace=T, p=c(1-p,p))
# Define `X_bar` as the average sampled proportion
    X_bar <- mean(X)
# Calculate the standard error of the estimate. Print the result to the console.
    sqrt(X_bar*(1-X_bar)/N)


Exercise 8. Plotting the standard error¶
The standard error estimates obtained from the Monte Carlo simulation, the theoretical prediction, 
and the estimate of the theoretical prediction are all very close, which tells us that the theory is working. 
This gives us a practical approach to knowing the typical error we will make if we predict p with X^. 
he theoretical result gives us an idea of how large a sample size is required to obtain the precision we need. 
Earlier we learned that the largest standard errors occur for p=0.5
Create a plot of the largest standard error for N ranging from 100 to 5,000. 
Based on this plot, how large does the sample size have to be to have a standard error of about 1%?
    N <- seq(100, 5000, len = 100) 
    p <- 0.5 
    e <- sqrt(p*(1-p)/N)
2,500

Exercise 11. Plotting the errors
Make a qq-plot of the errors you generated previously to see if they follow a normal distribution.
Run the supplied code
Use the qqnorm function to produce a qq-plot of the errors.
Use the qqline function to plot a line showing a normal distribution.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
    B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
    set.seed(1)
# Generate `errors` by subtracting the estimate from the actual proportion of Democratic voters
    errors <- replicate(B, p - take_sample(p, N))
# Generate a qq-plot of `errors` with a qq-line showing a normal distribution
    qqnorm(errors)
    qqline(errors)

Exercise 12. Estimating the probability of a specific value of X-bar¶
If p=0.45 and N=100, use the central limit theorem to estimate the probability that X¯>0.5.
# Define `p` as the proportion of Democrats in the population being polled
    p <- 0.45
# Define `N` as the number of people polled
    N <- 100
# Calculate the probability that the estimated proportion of Democrats in the population is greater than 0.5. Print this value to the console.
    1-pnorm(0.5, mean = p, sd=(sqrt(p*(1-p)/N)))

Exercise 13. Estimating the probability of a specific error size
Assume you are in a practical situation and you don't know p . 
Take a sample of size N=100 and obtain a sample average of X¯=0.51
What is the CLT approximation for the probability that your error is equal or larger than 0.01?
Calculate the standard error of the sample average using the sqrt function.
Use pnorm twice to define the probabilities that a value will be less than 0.01 or -0.01.
Calculate the probability that the error will be 0.01 or larger.
# Define `N` as the number of people polled
    N <-100
# Define `X_hat` as the sample average
    X_hat <- 0.51
# Define `se_hat` as the standard error of the sample average
    se_hat <- sqrt(X_hat*(1-X_hat)/N)
# Calculate the probability that the error is 0.01 or larger
    1 - pnorm(.01, 0, se_hat) + pnorm(-0.01, 0, se_hat)



### Confidence Intervals
Calculate confidence intervals of difference sizes around an estimate.
Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.
Explain the concept of "power" as it relates to inference.
Understand the relationship between p-values and confidence intervals and explain why 
reporting confidence intervals is often preferable.


Exercise 1. Confidence interval for p
For the following exercises, we will use actual poll data from the 2016 election. The exercises will contain pre-loaded data from the dslabs package.
library(dslabs)
data("polls_us_election_2016")
We will use all the national polls that ended within a few weeks before the election.
Assume there are only two candidates and construct a 95% confidence interval for the election night proportion p.
Use filter to subset the data set for the poll data you want. Include polls that ended on or after October 31, 2016 (enddate). Only include polls that took place in the United States. Call this filtered object polls.
Use nrow to make sure you created a filtered object polls that contains the correct number of rows.
Extract the sample size N from the first poll in your subset object polls.
Convert the percentage of Clinton voters (rawpoll_clinton) from the first poll in polls to a proportion, X_hat. Print this value to the console.
# Load the data
data("polls_us_election_2016")
# Generate an object `polls` that contains data filtered for polls that ended on or after October 31, 2016 in the United States
po
# How many rows does `polls` contain? Print this value to the console.
nrow(polls)
# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- polls$samplesize[1]
N
# For the first poll in `polls`, convert the percentage to a proportion of Clinton voters and assign it to a variable called `X_hat`. Print this value to the console.
X_hat <- polls$rawpoll_clinton[1]/100
X_hat
# Calculate the standard error of `X_hat` and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(X_hat*(1-X_hat)/N)
se_hat
# Use `qnorm` to calculate the 95% confidence interval for the proportion of Clinton voters. 
Save the lower and then the upper confidence interval to a variable called `ci`.
ci<- c(X_hat - qnorm(0.975)*se_hat, X_hat + qnorm(0.975)*se_hat)
ci <- c(d_hat - qnorm(0.975)*se_hat, d_hat + qnorm(0.975)*se_hat)

Exercise 2. Pollster results for p
Create a new object called pollster_results that contains the pollster's name, the end date of the poll, the proportion of voters who declared a vote for Clinton, the standard error of this estimate, and the lower and upper bounds of the confidence interval for the estimate.
Use the mutate function to define four new columns: X_hat, se_hat, lower, and upper. Temporarily add these columns to the polls object that has already been loaded for you.
In the X_hat column, convert the raw poll results for Clinton to a proportion.
In the se_hat column, calculate the standard error of X_hat for each poll using the sqrt function.
In the lower column, calculate the lower bound of the 95% confidence interval using the qnorm function.
In the upper column, calculate the upper bound of the 95% confidence interval using the qnorm function.
Use the select function to select the columns from polls to save to the new object pollster_results.
# The `polls` object that filtered all the data by date and nation has already been loaded. Examine it using the `head` function.
head(polls)
# Create a new object called `pollster_results` that contains columns for pollster name, end date, X_hat, se_hat, lower confidence interval, and upper confidence interval for each poll.
pollster_results <- polls %>% mutate(X_hat = polls$rawpoll_clinton/100, 
                se_hat = sqrt(X_hat*(1-X_hat)/samplesize), 
                lower = X_hat - qnorm(0.975)*se_hat, 
                upper = X_hat + qnorm(0.975)*se_hat) %>%
  select(pollster, enddate, X_hat, se_hat, lower, upper)
  pollster_results

Exercise 3. Comparing to actual results - p
The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. 
Add a column called hit to pollster_results that states if the confidence interval included the true 
proportion p=0.482 or not. What proportion of confidence intervals included p?
Use the mutate function to define a new variable called 'hit'.
Use logical expressions to determine if each values in lower and upper span the actual proportion.
Use the mean function to determine the average value in hit and summarize the results using summarize.
Save the result as an object called avg_hit.
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)
# Add a logical variable called `hit` that indicates whether the actual value exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <-pollster_results %>% mutate(hit = lower<=0.482 & upper>=0.482) %>%
  select(pollster, enddate, X_hat, lower, upper, hit) %>%
  summarize(mean(hit))

Exercise 5. Confidence interval for d
A much smaller proportion of the polls than expected produce confidence intervals containing p. Notice that most polls 
that fail to include p are underestimating. The rationale for this is that undecided voters historically divide evenly 
between the two main candidates on election day.
In this case, it is more informative to estimate the spread or the difference between the proportion of two candidates d, 
or 0.482−0.461=0.021 for this election.
Assume that there are only two parties and that d=2p−1. Construct a 95% confidence interval for difference in proportions on election night.
Use the mutate function to define a new variable called 'd_hat' in polls. The new variable subtract the proportion of Trump voters from the proportion of Clinton voters.
Extract the sample size N from the first poll in your subset object polls.
Extract the difference in proportions of voters d_hat from the first poll in your subset object polls.
Use the formula above to calculate p from d_hat. Assign p to the variable X_hat.
Find the standard error of the spread given N.
Calculate the 95% confidence interval of this estimate of the difference in proportions, d_hat, using the qnorm function.
Save the lower and upper confidence intervals as an object called ci. Save the lower confidence interval first.
# Add a statement to this line of code that will add a new column named `d_hat` to `polls`. The new column should contain the difference in the proportion of voters.
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100)
# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- polls$samplesize[1]
# For the difference `d_hat` of the first poll in `polls` to a variable called `d_hat`. Print this value to the console.
d_hat <- polls$d_hat[1]
d_hat
# Assign proportion of votes for Clinton to the variable `X_hat`.
X_hat <- (d_hat+1)/2
# Calculate the standard error of the spread and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- 2*sqrt(X_hat*(1-X_hat)/N)
se_hat
# Use `qnorm` to calculate the 95% confidence interval for the difference in the proportions of voters. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(d_hat - qnorm(0.975)*se_hat, d_hat + qnorm(0.975)*se_hat)

Exercise 6. Pollster results for d
Create a new object called pollster_results that contains the pollster's name, the end date of the poll, the difference in the proportion of voters who declared a vote either, the standard error of this estimate, and the lower and upper bounds of the confidence interval for the estimate.
Use the mutate function to define four new columns: 'X_hat', 'se_hat', 'lower', and 'upper'. Temporarily add these columns to the polls object that has already been loaded for you.
In the X_hat column, calculate the proportion of voters for Clinton using d_hat.
In the se_hat column, calculate the standard error of the spread for each poll using the sqrt function.
In the lower column, calculate the lower bound of the 95% confidence interval using the qnorm function.
In the upper column, calculate the upper bound of the 95% confidence interval using the qnorm function.
Use the select function to select the columns from polls to save to the new object pollster_results.
# The subset `polls` data with 'd_hat' already calculated has been loaded. Examine it using the `head` function.
head(polls)
# Create a new object called `pollster_results` that contains columns for pollster name, end date, d_hat, lower confidence interval of d_hat, and upper confidence interval of d_hat for each poll.
pollster_results <- polls %>% mutate(X_hat = (d_hat+1)/2, 
                 se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize),
                 lower = d_hat - qnorm(0.975)*se_hat, 
                 upper = d_hat + qnorm(0.975)*se_hat) %>%
  select(pollster, enddate, d_hat, lower, upper) 
  pollster_results

Exercise 7. Comparing to actual results - d
What proportion of confidence intervals for the difference between the proportion of voters included d, the actual difference in election day?
Use the mutate function to define a new variable within pollster_results called hit.
Use logical expressions to determine if each values in lower and upper span the actual difference in proportions of voters.
Use the mean function to determine the average value in hit and summarize the results using summarize.
Save the result as an object called avg_hit.
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)
# Add a logical variable called `hit` that indicates whether the actual value (0.021) exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <- polls %>% mutate(X_hat = (d_hat+1)/2, 
                  se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize),
                  lower = d_hat - qnorm(0.975)*se_hat, 
                  upper = d_hat + qnorm(0.975)*se_hat, 
                  hit = lower<=0.021 & upper>=0.021) %>%
  select(pollster, enddate, d_hat, lower, upper, hit) %>% 
  summarize(mean(hit))

Exercise 8. Comparing to actual results by pollster
Although the proportion of confidence intervals that include the actual difference between the proportion of voters increases substantially, it is still lower that 0.95. In the next chapter, we learn the reason for this.
To motivate our next exercises, calculate the difference between each poll's estimate d¯ and the actual d=0.021. Stratify this difference, or error, by pollster in a plot.
Define a new variable errors that contains the difference between the estimated difference between the proportion of voters and the actual difference on election day, 0.021.
To create the plot of errors by pollster, add a layer with the function geom_point. The aesthetic mappings require a definition of the x-axis and y-axis variables. So the code looks like the example below, but you fill in the variables for x and y.
The last line of the example code adjusts the x-axis labels so that they are easier to read.
data %>% ggplot(aes(x = , y = )) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)
# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster.
polls %>% mutate(error = d_hat - 0.021) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

Exercise 9. Comparing to actual results by pollster - multiple polls
Remake the plot you made for the previous exercise, but only for pollsters that took five or more polls.
You can use dplyr tools group_by and n to group data by a variable of interest and then count the number of observations in the groups. The function filter filters data piped into it by your specified condition.
For example:
data %>% group_by(variable_for_grouping) 
    %>% filter(n() >= 5)
Define a new variable errors that contains the difference between the estimated difference between the proportion of voters and the actual difference on election day, 0.021.
Group the data by pollster using the group_by function.
Filter the data by pollsters with 5 or more polls.
Use ggplot to create the plot of errors by pollster.
Add a layer with the function geom_point.
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)
# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster, but only for pollsters who took 5 or more polls.
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

###

Poll Aggregators

Understand how aggregating data from different sources, as poll aggregators do for poll data, can improve the precision
of a prediction.
Understand how to fit a multilevel model to the data to forecast, for example, election results.
Explain why a simple aggregation of data is insufficient to combine results because of factors such as pollster bias.
Use a data-driven model to account for additional types of sampling variability such as pollster-to-pollster 
variability

Exercise 1 - Heights Revisited
We have been using urn models to motivate the use of probability models. However, most data science applications are not related to data obtained from urns. More common are data that come from individuals. Probability plays a role because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.
Let's revisit the heights dataset. For now, consider x to be the heights of all males in the data set. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it.
What are the population average and standard deviation of our population?
Execute the lines of code that create a vector x that contains heights for all males in the population.
Calculate the average of x.
Calculate the standard deviation of x.
# Load the 'dslabs' package and data contained in 'heights'
library(dslabs)
data(heights)
# Make a vector of heights from all males in the population
x <- heights %>% filter(sex == "Male") %>%
  .$height
# Calculate the population average. Print this value to the console.
set.seed(1)
N <- 50
X <- sample(x, N, replace = TRUE)
mean(x)
# Calculate the population standard deviation. Print this value to the console.
sd(x)

Exercise 2 - Sample the population of heights
Call the population average computed above μ and the standard deviation σ. Now take a sample of size 50, with replacement, and construct an estimate for μ and σ.
Use the sample function to sample N values from x.
Calculate the mean of the sampled heights.
Calculate the standard deviation of the sampled heights.
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)
# Calculate the sample average. Print this value to the console.
mean(X)
# Calculate the sample standard deviation. Print this value to the console.
sd(X)


Exercise 4 - Confidence Interval Calculation
We will use X¯ as our estimate of the heights in the population from our sample size N. 
We know from previous exercises that the standard estimate of our error X¯−μ is σ/N−−√.
Construct a 95% confidence interval for μ.
Exercise 4 - Confidence Interval Calculation
We will use X¯ as our estimate of the heights in the population from our sample size N. 
We know from previous exercises that the standard estimate of our error X¯−μ is σ/N−−√.
Construct a 95% confidence interval for μ.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)
# Define `se` as the standard error of the estimate. Print this value to the console.
se <- sd(X)/sqrt(N)
se
# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(mean(X) - qnorm(0.975)*se, mean(X) + qnorm(0.975)*se)

Exercise 5 - Monte Carlo Simulation for Heights
Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include μ?
Use the replicate function to replicate the sample code for B <- 10000 simulations. Save the results of the replicated code to a variable called res. The replicated code should complete the following steps: -1. Use the sample function to sample N values from x. Save the sampled heights as a vector called X. -2. Create an object called interval that contains the 95% confidence interval for each of the samples. Use the same formula you used in the previous exercise to calculate this interval. -3. Use the between function to determine if μ is contained within the confidence interval of that simulation.
Finally, use the mean function to determine the proportion of results in res that contain mu.
# Define `mu` as the population average
mu <- mean(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `B` as the number of times to run the model
B <- 10000
# Define an object `res` that contains a logical vector for simulated intervals that contain mu
mu <- mean(x)
set.seed(1)
N <- 50
B <- 10000
res <- replicate(B, {
  X <- sample(x, N, replace=TRUE)
  interval <- mean(X) + c(-1,1)*qnorm(0.975)*sd(X)/sqrt(N)
  between(mu, interval[1], interval[2])
})
# Calculate the proportion of results in `res` that include mu. Print this value to the console.
mean(res)

Exercise 6 - Visualizing Polling Bias
In this section, we used visualization to motivate the presence of pollster bias in election polls. Here we will examine that bias more rigorously. Lets consider two pollsters that conducted daily polls and look at national polls for the month before the election.
Is there a poll bias? Make a plot of the spreads for each poll.
Use ggplot to plot the spread for each of the two pollsters.
Define the x- and y-axes usingusing aes() within the ggplot function.
Use geom_boxplot to make a boxplot of the data.
Use geom_point to add data points to the plot.
# Load the libraries and data you need for the following exercises
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")
# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>% 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 
# Make a boxplot with points of the spread for each pollster
polls %>% ggplot(aes(pollster, spread)) + 
  geom_boxplot() + 
  geom_point()


b1 != b2
d + b1
The expected value is d+b1 and the standard error is σ1/sqrt N1
The expected value is d+b2 and the standard error is σ2/sqrt(N2)
b2 - b1
sqrt(σ2,2/N2+σ21/N1)

Exercise 13 - Compute the Estimates
The answer to the previous question depends on σ1 and σ2, which we don't know. We learned that we can estimate these values using the sample standard deviation.
Compute the estimates of σ1 and σ2.
Group the data by pollster.
Summarize the standard deviation of the spreads for each of the two pollsters.
Store the pollster names and standard deviations of the spreads (σ) in an object called sigma
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)
# Create an object called `sigma` that contains a column for `pollster` and a column for `s`, the standard deviation of the spread
sigma <- polls %>% group_by(pollster) %>%
  summarize(s = sd(spread))
# Print the contents of sigma to the console
sigma

If we assume N2 and N1 are large enough, Y¯2 and Y¯1, and their difference, are approximately normal.

Exercise 15 - Calculate the 95% Confidence Interval of the Spreads
We have constructed a random variable that has expected value b2−b1, the pollster bias difference. 
If our model holds, then this random variable has an approximately normal distribution. The standard error of 
this random variable depends on σ1 and σ2, but we can use the sample standard deviations we computed earlier. 
We have everything we need to answer our initial question: is b2−b1 different from 0?
Construct a 95% confidence interval for the difference b2 and b1. Does this interval contain zero?
Use pipes %>% to pass the data polls on to functions that will group by pollster and summarize the average spread,
standard deviation, and number of polls per pollster.
Calculate the estimate by subtracting the average spreads.
Calculate the standard error using the standard deviations of the spreads and the sample size.
Calculate the 95% confidence intervals using the qnorm function.
Save the lower and then the upper confidence interval to a variable called ci.
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)
# Create an object called `res` that summarizes the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 
# Store the difference between the larger average and the smaller in a variable called `estimate`. Print this value to the console.
estimate <- res$avg[2] - res$avg[1]
estimate
# Store the standard error of the estimates as a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat
# Calculate the 95% confidence interval of the spreads. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(estimate - qnorm(0.975)*se_hat, estimate + qnorm(0.975)*se_hat)

Exercise 16 - Calculate the P-value
The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. 
Random variability does not seem to explain it.
Compute a p-value to relay the fact that chance does not explain the observed pollster effect.
Use the pnorm function to calculate the probability that a random value is larger than the observed ratio of the estimate
to the standard error.
Multiply the probability by 2, because this is the two-tailed test.
# We made an object `res` to summarize the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 

# The variables `estimate` and `se_hat` contain the spread estimates and standard error, respectively.
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])

# Calculate the p-value
2*(1 - pnorm(estimate/se_hat, 0, 1))


Exercise 17 - Comparing Within-Poll and Between-Poll Variability
We compute statistic called the t-statistic by dividing our estimate of b2−b1 by its estimated standard error:
Y¯2−Y¯1 / sqrt(s2,2/N2+s21/N1)
Later we learn will learn of another approximation for the distribution of this statistic for values of N2 and N1 that aren't large enough for the CLT.
Note that our data has more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?
Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.
Group the polls data by pollster.
Summarize the average and standard deviation of the spreads for each pollster.
Create an object called var that contains three columns: pollster, mean spread, and standard deviation.
Be sure to name the column for mean avg and the column for standard deviation s.
# Execute the following lines of code to filter the polling data and calculate the spread
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ungroup()
# Create an object called `var` that contains columns for the pollster, mean spread, and standard deviation. Print the contents of this object to the console.
var <- polls %>% group_by(pollster) %>%
  summarize(avg = mean(spread), s = sd(spread))
var



Exercise 1 - Confidence Intervals of Polling Data
# Load the libraries and data
library(dplyr)
library(dslabs)
data("polls_us_election_2016")

# Create a table called `polls` that filters by  state, date, and reports the spread
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Create an object called `cis` that columns for the lower and upper confidence intervals. Select the columns indicated in the instructions.
cis <- polls %>% mutate(X_hat = (spread+1)/2, se = 2*sqrt(X_hat*(1-X_hat)/samplesize), 
                 lower = spread - qnorm(0.975)*se, upper = spread + qnorm(0.975)*se) %>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)

Exercise 2 - Compare to Actual Results
# Add the actual results to the `cis` data set
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of confidence intervals that contain the actual value. Print this object to the console.
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% summarize(proportion_hits = mean(hit))
p_hits

3
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
# Create an object called `p_hits` that summarizes the proportion of hits for each pollster that has at least 5 polls. 
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(pollster) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n(), grade = grade[1]) %>%
  arrange(desc(proportion_hits))
p_hits

4
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls. 
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n()) %>%
  arrange(desc(proportion_hits)) 
p_hits

5
# The `p_hits` data have already been loaded for you. Use the `head` function to examine it.
head(p_hits)

# Make a barplot of the proportion of hits for each state
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
  coord_flip()

6
# The `cis` data have already been loaded. Examine it using the `head` function.
head(cis)

# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Examine the last 6 rows of `errors`
tail(errors)

7
# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls
p_hits <- errors %>%  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n())

# Make a barplot of the proportion of hits for each state
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
 coord_flip()


8
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Generate a histogram of the error
hist(errors$error)

# Calculate the median of the errors. Print this value to the console.
median(errors$error)

9
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Create a boxplot showing the errors by state for polls with grades B+ or higher
errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()

10
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Create a boxplot showing the errors by state for states with at least 5 polls with grades B+ or higher
errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  group_by(state) %>%
  filter(n() >= 5) %>%
  ungroup() %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()















































